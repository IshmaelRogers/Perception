# Ishmael Rogers
# Robotics Software Engineer, Infinitely Deep Robotics Group
# www.idrg.io
# Atlanta, Ga 
# 2018 

[image1]: ./images/k-means.png
[image2]: ./images/clustercompare.png
[image3]: ./images/DBSCAN.png
      

# Clustering for Segmentation 

# Object Segmentation 

We know that segmentation is the process of dividing point cloud data into meaningful subsets using a common property.

After filtering point cloud data and apply RANSAC to remove usless information from the scene. Now we will.. 
Combine color information with shape information to perform complex segmentation task.

Clustering allow us to segment objects in point cloud without assuming a model shape. FInd points in dataset that are clustered together based on certain a feature (or collection of features)

0. Color
1. Shape 
2. Position
3. Texture 

# Downside of Model Fitting

Relying solely on the RANSAC algorithm to segment objects in a scene is not always ideal if you are dealing with a cluttered environment with little or no prior information about the number of objects in the scene. Furthermore, this method can induce false positives if there are many objects present that could fit the same shape model as the target object. For example, if my target object is a can of soda, a scene filled with other cylindrical objects can cause the robot to become confused.   

A scene that has objects of various shapes also present a problem when using the RANSAC approach. Recall, in the RANSAC algorithm we specified a model to be removed. If the image taken in by the system has different types of shapes, a model would need to be created and processed individually. This would require a lot of computation power. 

RGB-D cameras provide data that is rich in features such as color and texture. We now explore the process of using these features to segment the objects we are looking for.

# Clustering 

Is the process of finding similarities among individual points in some point cloud data so that they may be segmented. By clustering data we provide our robot with a way to determine which components of a dataset naturally belong together.
NOTE: Clustering can be performed based on spatial neighborhood as well as color. 

In this repository we will use two different approaches for clustering

 0. K-means
 - An iterative, unsupervised learning algorithm works by dividing the input data into a key number of clusters based on one or more features or properties.
1. Euclidean
 - A clustering technique by which points that are closer to each other are clustered together by making use of a 3D grid subdivision of the space.

# K-means Clustering 
K-means clustering is the most simple and widely used cluster technique clustering technique in computer vision, machine learning and data mining.

![alt text][image1]

The Algorithm
---
Given a dataset containing n number of points <a href="https://www.codecogs.com/eqnedit.php?latex=(p_{1},&space;p_{2},\cdots,&space;p_{n}&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(p_{1},&space;p_{2},\cdots,&space;p_{n}&space;)" title="(p_{1}, p_{2},\cdots, p_{n} )" /></a> plotted based on two or more features, we would like to cluster the points together into k different clusters: 

 0. Randomly select k individual point as the initial cluster centroids: <a href="https://www.codecogs.com/eqnedit.php?latex=(c_{1},&space;c_{2},\cdots,&space;c_{k}&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(c_{1},&space;c_{2},\cdots,&space;c_{k}&space;)" title="(c_{1}, c_{2},\cdots, c_{k} )" /></a>
 1. Define convergance/termination criteria based on solution stabillity and max number of iterations.
 2. While convergence/termination criteria are not met:
    for i = 1 to n:
        Calculate the distance from point <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> to each cluster centroid.
        Assign <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> to its closest centroid and label it accordingly
        NOTE: The partition created in this step represent a Voronoi diagram generated by the centroids 
        end for loop
 3. For j = 1 to k:
        Recompute the centroid of cluster j based on the average of all data points that belong to the cluster.
         NOTE: The centroids <a href="https://www.codecogs.com/eqnedit.php?latex=(c_{1},&space;c_{2},\cdots,&space;c_{k}&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(c_{1},&space;c_{2},\cdots,&space;c_{k}&space;)" title="(c_{1}, c_{2},\cdots, c_{k} )" /></a> will shift to the new location 
    endfor
 endwhile 
 
The process of clustering will result in k number of different items grouped from the dataset clustered together based on the features specifed on the axis of the plot.

Testing k-means
---
Use the code in this repo to test the limitations of the k-means clustering algorithm.

Functions
--
0. cluster_gen() - generates simple cluster populations drawn from random Gaussian distributions. It contains tuneable parameters that allows us to change the distribution of cluster generated. 
NOTE: the np.random package has more information on how the functions work and other distributions that can be drawn from. 
1. cv2.kmeans() - converts the data into a format compatiable with OpenCV and defines the constrains for the k-means algorithm.

Code for extracting the individual clusters for plotting: 
---
#Define some empty lists to receive k-means cluster points
kmeans_clusters_x = []
kmeans_clusters_y = []

#Extract k-means clusters from output
for idx in range (k_clusters):
    kmeans_clusters_x.append(data[label.ravel()==idx][:,0])
    kmeans_clusters_y.append(data[label.ravel()==idx][:,1])
#Plot
# Plot up a comparison of original clusters vs. k-means clusters
fig = plt.figure(figsize=(12,6))
plt.subplot(121)
min_x = np.min(data[:, 0])
max_x = np.max(data[:, 0])
min_y = np.min(data[:, 1])
max_y = np.max(data[:, 1])
for idx, xpts in enumerate(clusters_x): 
    plt.plot(xpts, clusters_y[idx], 'o')
    plt.xlim(min_x, max_x)
    plt.ylim(min_y, max_y)
    plt.title('Original Clusters')
plt.subplot(122)
for idx, xpts in enumerate(kmeans_clusters_x):
    plt.plot(xpts, kmeans_clusters_y[idx], 'o')
    plt.xlim(min_x, max_x)
    plt.ylim(min_y, max_y)
    plt.title('k-means Clusters')
The results of this code: 
![alt text][image2]

Tuneable parameters
---
n_clusters = number of clusters to generate
pts_minmax = range of number of points per cluster 
x_mult = range of multiplier to modify the size of cluster in the x-direction
y_mult = range of multiplier to modify the size of cluster in the y-direction
x_off = range of cluster position offset in the x-direction
y_off = range of cluster position offset in the y-direction
    

# DBSCAN Algorithm

(Density-Based Spatial CLustering of Applications with Noise) aka Euclidean Clustering

![alt text][image3]

This clustering algorithm is an alternative to k-means when the number of clusters to expect in the dataset is unknown but something about how the points should be clustered in terms of density (distance between points in a cluster). 

It works by creating clusters by grouping data points that are within some threshold distance, <a href="https://www.codecogs.com/eqnedit.php?latex=d_{th}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?d_{th}" title="d_{th}" /></a> from the nearest other point in the data. 

The decision of whether to place a point in a cluster is based on the Euclidean distance between that point and other cluster members

NOTE: The Euclidean distance is the length of a line connecting two points. The coordniates defining the positions of points in the dataset do no need to be spatial coordinates. 

Given points p and q in an n-dimensional dataset where the position of p and q are defined as follows: 

<a href="https://www.codecogs.com/eqnedit.php?latex=(p_{1},&space;p_{2},\cdots,&space;p_{n})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(p_{1},&space;p_{2},\cdots,&space;p_{n})" title="(p_{1}, p_{2},\cdots, p_{n})" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=(q_{1},&space;q_{2},\cdots,&space;q_{n})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(q_{1},&space;q_{2},\cdots,&space;q_{n})" title="(q_{1}, q_{2},\cdots, q_{n})" /></a>

The distance between the two point is:

<a href="https://www.codecogs.com/eqnedit.php?latex=D&space;=&space;\sqrt{(p_{1}-q_{1})^{2}&space;&plus;&space;(p_{2}-q_{2})^{2}&plus;\cdots&space;&plus;&space;(p_{n}-q_{n})^2)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?D&space;=&space;\sqrt{(p_{1}-q_{1})^{2}&space;&plus;&space;(p_{2}-q_{2})^{2}&plus;\cdots&space;&plus;&space;(p_{n}-q_{n})^2)}" title="D = \sqrt{(p_{1}-q_{1})^{2} + (p_{2}-q_{2})^{2}+\cdots + (p_{n}-q_{n})^2)}" /></a>

Steps for DBSCAN Clustering
--
Given a set P of n data points <a href="https://www.codecogs.com/eqnedit.php?latex=p_{1},&space;p_{2},&space;\cdots&space;,&space;p_{n}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{1},&space;p_{2},&space;\cdots&space;,&space;p_{n}" title="p_{1}, p_{2}, \cdots , p_{n}" /></a> : 

0. Set contrainsts for the minimum number of points that make up a cluster (min_samples)
1. Set the distance maximum distance between cluster points (max_dist)
2. For ever point <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> in P:
      if <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> has at least one neighbor within max_dist:
            if <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a>'s neighbor is part of a cluster 
                  add <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> to that cluster
            if <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> has at least min_samples -1 neighbors within max_dist:
                  <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> becomes a "core member" of the cluster
            else: 
            <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> becomes an "edge member" of the cluster
     else:
            <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> is defined as an outlier.
        

We need to control the order in which we visit data points to avoid visiting every data point at random and creating a lot more clusters than needed. To do this, each time we create a new cluster with our first qualifying point, we will add all of its neighbors to the cluster. Then we add its neighbor's neighbor's and their neighbors until we have visited every data point that belongs to this cluster.

NOTE: This step is crucial and only after do it, we may move on to choosing another data point at random. Doing this will guarantee that a cluster is complete and no more points in the data set belong to that cluster. See DBSCAN Wiki for pseudo-code implementation

Take a look at DBSCAN_test.py in this repo!


# DBSCAN vs K-means

# Clustering with PCL 

# Publishing a Point Cloud 

# Flitering and RANSAC

# Clustering Objects

# Cluster Visualization



