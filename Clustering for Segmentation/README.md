# Ishmael Rogers
# Robotics Software Engineer, Infinitely Deep Robotics Group
# www.idrg.io
# Atlanta, Ga 
# 2018 

[image1]: ./images/k-means.png
[image2]: ./images/clustercompare.png
      

# Clustering for Segmentation 

# Object Segmentation 

We know that segmentation is the process of dividing point cloud data into meaningful subsets using a common property.

After filtering point cloud data and apply RANSAC to remove usless information from the scene. Now we will.. 
Combine color information with shape information to perform complex segmentation task.

Clustering allow us to segment objects in point cloud without assuming a model shape. FInd points in dataset that are clustered together based on certain a feature (or collection of features)

0. Color
1. Shape 
2. Position
3. Texture 

# Downside of Model Fitting

Relying solely on the RANSAC algorithm to segment objects in a scene is not always ideal if you are dealing with a cluttered environment with little or no prior information about the number of objects in the scene. Furthermore, this method can induce false positives if there are many objects present that could fit the same shape model as the target object. For example, if my target object is a can of soda, a scene filled with other cylindrical objects can cause the robot to become confused.   

A scene that has objects of various shapes also present a problem when using the RANSAC approach. Recall, in the RANSAC algorithm we specified a model to be removed. If the image taken in by the system has different types of shapes, a model would need to be created and processed individually. This would require a lot of computation power. 

RGB-D cameras provide data that is rich in features such as color and texture. We now explore the process of using these features to segment the objects we are looking for.

# Clustering 

Is the process of finding similarities among individual points in some point cloud data so that they may be segmented. By clustering data we provide our robot with a way to determine which components of a dataset naturally belong together.
NOTE: Clustering can be performed based on spatial neighborhood as well as color. 

In this repository we will use two different approaches for clustering

 0. K-means
 - An iterative, unsupervised learning algorithm works by dividing the input data into a key number of clusters based on one or more features or properties.
1. Euclidean
 - A clustering technique by which points that are closer to each other are clustered together by making use of a 3D grid subdivision of the space.

# K-means Clustering 
K-means clustering is the most simple and widely used cluster technique clustering technique in computer vision, machine learning and data mining.

![alt text][image1]

The Algorithm
---
Given a dataset containing n number of points <a href="https://www.codecogs.com/eqnedit.php?latex=(p_{1},&space;p_{2},\cdots,&space;p_{n}&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(p_{1},&space;p_{2},\cdots,&space;p_{n}&space;)" title="(p_{1}, p_{2},\cdots, p_{n} )" /></a> plotted based on two or more features, we would like to cluster the points together into k different clusters: 

 0. Randomly select k individual point as the initial cluster centroids: <a href="https://www.codecogs.com/eqnedit.php?latex=(c_{1},&space;c_{2},\cdots,&space;c_{k}&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(c_{1},&space;c_{2},\cdots,&space;c_{k}&space;)" title="(c_{1}, c_{2},\cdots, c_{k} )" /></a>
 1. Define convergance/termination criteria based on solution stabillity and max number of iterations.
 2. While convergence/termination criteria are not met:
    for i = 1 to n:
        Calculate the distance from point <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> to each cluster centroid.
        Assign <a href="https://www.codecogs.com/eqnedit.php?latex=p_{i}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?p_{i}" title="p_{i}" /></a> to its closest centroid and label it accordingly
        NOTE: The partition created in this step represent a Voronoi diagram generated by the centroids 
        end for loop
 3. For j = 1 to k:
        Recompute the centroid of cluster j based on the average of all data points that belong to the cluster.
         NOTE: The centroids <a href="https://www.codecogs.com/eqnedit.php?latex=(c_{1},&space;c_{2},\cdots,&space;c_{k}&space;)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(c_{1},&space;c_{2},\cdots,&space;c_{k}&space;)" title="(c_{1}, c_{2},\cdots, c_{k} )" /></a> will shift to the new location 
    endfor
 endwhile 
 
The process of clustering will result in k number of different items grouped from the dataset clustered together based on the features specifed on the axis of the plot.

Testing k-means
---
Use the code in this repo to test the limitations of the k-means clustering algorithm.

Functions
--
0. cluster_gen() - generates simple cluster populations drawn from random Gaussian distributions. It contains tuneable parameters that allows us to change the distribution of cluster generated. 
NOTE: the np.random package has more information on how the functions work and other distributions that can be drawn from. 
1. cv2.kmeans() - converts the data into a format compatiable with OpenCV and defines the constrains for the k-means algorithm.

Code for extracting the individual clusters for plotting: 
---
#Define some empty lists to receive k-means cluster points
kmeans_clusters_x = []
kmeans_clusters_y = []

#Extract k-means clusters from output
for idx in range (k_clusters):
    kmeans_clusters_x.append(data[label.ravel()==idx][:,0])
    kmeans_clusters_y.append(data[label.ravel()==idx][:,1])
#Plot
# Plot up a comparison of original clusters vs. k-means clusters
fig = plt.figure(figsize=(12,6))
plt.subplot(121)
min_x = np.min(data[:, 0])
max_x = np.max(data[:, 0])
min_y = np.min(data[:, 1])
max_y = np.max(data[:, 1])
for idx, xpts in enumerate(clusters_x): 
    plt.plot(xpts, clusters_y[idx], 'o')
    plt.xlim(min_x, max_x)
    plt.ylim(min_y, max_y)
    plt.title('Original Clusters')
plt.subplot(122)
for idx, xpts in enumerate(kmeans_clusters_x):
    plt.plot(xpts, kmeans_clusters_y[idx], 'o')
    plt.xlim(min_x, max_x)
    plt.ylim(min_y, max_y)
    plt.title('k-means Clusters')
The results of this code: 
![alt text][image2]

Tuneable parameters
---
n_clusters = number of clusters to generate
pts_minmax = range of number of points per cluster 
x_mult = range of multiplier to modify the size of cluster in the x-direction
y_mult = range of multiplier to modify the size of cluster in the y-direction
x_off = range of cluster position offset in the x-direction
y_off = range of cluster position offset in the y-direction
    

# DBSCAN Algorithm

# DBSCAN vs K-means

# Clustering with PCL 

# Publishing a Point Cloud 

# Flitering and RANSAC

# Clustering Objects

# Cluster Visualization



